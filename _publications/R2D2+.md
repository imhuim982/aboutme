---
title: "Augmenting Transformers with Recursively Composed Multi-Grained Representations"
collection: publications
category: conferences
excerpt: 'We reduce the space complexity of the deep inside-outside algorithm from cubic to linear and further reduce the parallel time complexity to approximately log N thanks to the new pruning algorithm proposed in this paper. Furthermore, we find that joint pre-training of Transformers and composition models can enhance a variety of NLP downstream tasks. We push unsupervised constituency parsing performance to 65% and demonstrate that our model could outperform vanillar Trasformers around 5% on span-level tasks.'
venue: 'ICLR'
date: 2024-01-10
paperurl: 'https://aclanthology.org/2024.acl-long.145.pdf'
---
