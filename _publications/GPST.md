---
title: "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale"
collection: publications
category: conferences
excerpt: 'We propose GPST, a syntactic language model which could be pre-trained on raw text efficiently without any human-annotated trees. When GPST and GPT-2 are both pre-trained on OpenWebText from scratch, GPST can outperform GPT-2 on various downstream tasks. Moreover, it significantly surpasses previous methods on generative grammar induction tasks, exhibiting a high degree of consistency with human syntax.'
venue: 'ACL'
date: 2024-05-01
paperurl: 'https://openreview.net/pdf?id=MLJ5TF5FtXH'
---
